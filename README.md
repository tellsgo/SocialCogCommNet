# 社会认知通信网络（SocialCogCommNet）

社会认知通信网络是一个多智能体强化学习框架，旨在模拟具有社会认知能力的智能体之间的通信和协作。该项目实现了几种不同难度的协作游戏，并提供了一个具有社会认知能力的通信网络模型，用于智能体之间的有效协作。

## 项目结构

项目主要包含以下组件：

### 游戏

- `games/simple_coordination.py`: 简单协调游戏，两个智能体需要选择匹配的动作
- `games/asymmetric_info.py`: 非对称信息游戏，智能体拥有不同的信息，需要通过通信共享
- `games/sequential_decision.py`: 序列决策游戏，智能体需要在多个步骤中协调行动
- `games/partial_observable.py`: 部分可观察游戏，每个智能体只能观察到环境的一部分

### 核心组件

- `model.py`: 社会认知通信网络的实现，包括社会记忆、通信机制和理论心智
- `environment.py`: 协作环境实现，管理游戏状态和提供统一接口
- `games/state_processor.py`: 状态处理器，将不同游戏的状态转换为统一表示
- `games/game_manager.py`: 游戏管理器，支持课程学习和游戏切换

### 训练和测试脚本

- `main.py`: 主程序入口，用于运行训练和测试
- `train.py`: 训练脚本，使用Q学习训练智能体
- `test_deeper.py`: 深度训练和评估脚本，分析通信的重要性
- `test_all_games_simple.py`: 简单测试所有游戏的脚本
- `train_ppo.py`: PPO算法的实现（待完善）

## 运行指南

要运行基本的训练和测试，请使用：

```bash
python main.py
```

要进行深度训练和通信分析，请使用：

```bash
python test_deeper.py
```

要测试所有游戏，请使用：

```bash
python test_all_games_simple.py
```

## 未来改进方向

这个项目目前已经实现了基本功能，但还有以下几个方向可以进一步改进：

1. **PPO算法实现**：目前我们已经提供了PPO算法的框架，但还需要进一步调整以与我们的模型架构完全兼容。PPO是一种现代的策略梯度算法，相比Q学习更适合多智能体环境。

2. **通信可视化增强**：可以进一步改进通信内容的可视化，使用更复杂的工具（如t-SNE）来分析通信向量的聚类情况，帮助理解智能体是如何发展通信协议的。

3. **更复杂的游戏环境**：可以添加更多不同类型的游戏，例如竞争性游戏、混合动机游戏等，研究智能体在更复杂场景下的通信策略。

4. **增加智能体数量**：扩展框架以支持更多智能体，研究大规模多智能体系统中的通信和协作。

5. **通信约束**：添加通信约束（如有限带宽、通信延迟等），研究智能体在资源受限情况下如何优化通信。

6. **迁移学习研究**：深入研究课程学习和不同游戏之间的知识迁移，探索如何更有效地提高学习效率。

7. **社会记忆机制的研究**：深入分析社会记忆在长期交互中的作用，研究记忆如何影响智能体的决策和通信策略。

## 依赖项

- Python 3.8+
- PyTorch 1.8+
- NumPy
- Matplotlib
- Seaborn (用于可视化)

## 游戏说明

本项目实现了四种不同复杂度的游戏，用于研究智能体之间的通信和协作：

1. **简单协调游戏 (SimpleCoordinationGame)**：两个智能体需要选择相同的动作以获得奖励。
2. **非对称信息游戏 (AsymmetricInfoGame)**：两个智能体拥有不同的观察，需要通过通信协调行动。
3. **序列决策游戏 (SequentialDecisionGame)**：智能体需要在多个步骤中做出决策，考虑长期收益。
4. **部分可观察游戏 (PartialObservableGame)**：每个智能体只能观察到环境状态的一部分，需要通过通信共享信息。

## 模型说明

SocialCognitiveCommNet 是一个基于社会认知的通信网络，包含以下组件：

- **编码网络**：将观察编码为隐藏状态
- **通信生成器**：产生通信消息
- **通信处理器**：处理接收到的通信
- **社会记忆**：追踪其他智能体的行为模式
- **决策网络**：根据隐藏状态和社会记忆做出决策
- **价值网络**：估计状态价值

## 使用方法

### 安装依赖

```bash
pip install torch numpy matplotlib seaborn pandas
```

### 训练模型

基本训练：

```bash
python main.py --mode train
```

使用课程学习：

```bash
python main.py --mode train --curriculum
```

### 测试模型

```bash
python main.py --mode test --model_path results/20230101_120000/model_final.pt
```

测试特定游戏：

```bash
python test.py --model_path results/20230101_120000/model_final.pt --curriculum
```

### 配置参数

可以通过修改 `config.json` 文件来调整训练参数：

```json
{
    "hidden_dim": 128,       // 隐藏层维度
    "comm_dim": 64,          // 通信维度
    "memory_dim": 64,        // 社会记忆维度
    "n_episodes": 5000,      // 训练回合数
    "max_steps": 20,         // 每回合最大步数
    "batch_size": 64,        // 批次大小
    "buffer_capacity": 10000, // 经验回放缓冲区容量
    "learning_rate": 1e-4,   // 学习率
    "gamma": 0.99,           // 折扣因子
    "epsilon_start": 1.0,    // 初始探索率
    "epsilon_end": 0.05,     // 最终探索率
    "epsilon_decay": 10000,  // 探索率衰减
    "grad_clip": 1.0,        // 梯度裁剪
    "save_dir": "results",   // 结果保存目录
    "print_interval": 10,    // 打印间隔
    "eval_interval": 100,    // 评估间隔
    "use_cuda": true,        // 是否使用CUDA
    "seed": 42               // 随机种子
}
```

## 结果分析

训练过程中会生成以下可视化结果：

- 训练奖励曲线
- 训练损失曲线
- 任务成功率曲线
- 通信提升曲线
- 通信内容热力图
- 课程学习进展图
- 多游戏性能比较图

## 课程学习

课程学习模式下，模型会从简单游戏开始训练，逐步过渡到更复杂的游戏。当模型在当前游戏上达到一定性能（成功率超过80%）时，会自动切换到下一个游戏。

## 通信分析

系统提供了多种工具来分析智能体之间的通信：

- 通信内容可视化
- 通信演化分析
- 状态-通信相关性分析

## 扩展

要添加新的游戏，需要：

1. 创建一个继承自 `BaseGame` 的新游戏类
2. 实现必要的方法：`__init__`, `reset`, `step`, `get_test_scenarios`
3. 在 `game_manager.py` 中注册新游戏 

## game设计原则
- 游戏设计应遵循由简单到复杂的原则
- 每个游戏应该专注于训练一个特定的能力
- 游戏难度应该可调节，以适应不同阶段的学习
- 游戏之间应该有明确的技能依赖关系
- 游戏应该提供足够的反馈信息，帮助智能体学习
- 游戏场景应该具有一定的随机性，避免过拟合
- 游戏奖励设计要保持一致的风格，便于比较和分析
- 游戏状态空间通过状态调节器统一维度
- 游戏动作空间也需要统一维度和表示方式
- 游戏通信机制要保持一致，便于观察通信演化


## 下一步改进建议

添加更多算法：实现A2C、MADDPG和SAC算法，丰富算法库。
增强通信可视化：使用t-SNE和时间序列分析来可视化通信模式。
优化训练效率：通过并行环境支持和优先经验回放来提高训练效率。